{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the List of Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching we receive a huge list of results, then we have to rank the results and return the most informative!\n",
    "\n",
    "## Number of overlapping words:\n",
    "- not normalized by length of document\n",
    "\n",
    "## Jaccard Coefficient\n",
    "- $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/pgencheva/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories:  ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "Housing articles: ['test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/11170', 'training/11665', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/9615']\n",
      "\n",
      "Words in an arbitrary article: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "# NLTK supports access to different datasets https://www.nltk.org/book/ch02.html\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "print(\"\\nCategories: \", reuters.categories())\n",
    "\n",
    "housing_articles = reuters.fileids('housing')\n",
    "print(\"\\nHousing articles:\", housing_articles)\n",
    "\n",
    "print(\"\\nWords in an arbitrary article:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "Long Document words number: 131\n",
      "Long Document overlap: 3\n",
      "Long Document JC: 0.03571428571428571\n",
      "\n",
      "Short Document words number: 7\n",
      "Short Document overlap: 3\n",
      "Short Document JC: 0.375\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def word_overlap(doc_tokens, query_tokens):\n",
    "    return sum([1 for _tok in query_tokens if _tok in doc_tokens])\n",
    "\n",
    "def jaccard_coeff(doc_tokens, query_tokens):\n",
    "    # naive intersection of sets\n",
    "    return len(set(doc_tokens).intersection(set(query_tokens))) / len(set(doc_tokens).union(set(query_tokens)))\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"Long Document words number:\", len(reuters.words('training/6067')))\n",
    "print(\"Long Document overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Long Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document words number:\", len(short_similar_document))\n",
    "print(\"Short Document overlap:\", word_overlap(query_tokens, short_similar_document))\n",
    "print(\"Short Document JC:\", jaccard_coeff(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__But we also want to__:\n",
    "- Give _more weight_ to _less frequent words_ in the documents - __Balridge, prices__\n",
    "- Give _less weight_ to _more frequent words_ in the documents - __how, much, housing, to__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document word overlap: 13\n",
      "Document JC: 0.06593406593406594\n",
      "Document Content: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "query_tokens = tokenizer.tokenize('how much will the housing go up in the next month according to Balridge?')\n",
    "print(\"Document word overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document Content:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - Term Frequency- Inverted Document Frequency\n",
    "- View documents as __Bags Of Words__\n",
    "- Mary lent John some money. = John lent Mary some money.\n",
    "- Formula: \n",
    "\n",
    "$$TF * IDF (word, document) = (1+log(tf(word, document)) * log(\\frac{n}{df(word)})$$\n",
    "- n - total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "- __Frequency of word in a document (here, raw count)__\n",
    "- __0 if the term is not met in the document!!!__\n",
    "- Relevance does not increase proportionally with frequency -> __log (base of 10)__\n",
    "- Makes TF-IDF __increase with the number of occurrences__ within a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>34</td>\n",
       "      <td>2.531479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>31</td>\n",
       "      <td>2.491362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>21</td>\n",
       "      <td>2.322219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pct</td>\n",
       "      <td>15</td>\n",
       "      <td>2.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.113943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token  freq        tf\n",
       "0     .    34  2.531479\n",
       "1     ,    31  2.491362\n",
       "2    in    21  2.322219\n",
       "3   pct    15  2.176091\n",
       "4     1    13  2.113943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>below</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>level</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>687</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  freq   tf\n",
       "110    689     1  1.0\n",
       "111     11     1  1.0\n",
       "112  below     1  1.0\n",
       "113  level     1  1.0\n",
       "114    687     1  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pandas\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(Counter(reuters.words('test/20116')).most_common(), columns=['token', 'freq'])\n",
    "df['tf'] = 1 + np.log10(df['freq'])\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency\n",
    "- __Number of documents containing the word__ - an inversed measure of significance\n",
    "- Logarithm with base 10 dampens the effect of IDF\n",
    "- Affects ranking of queries with __at least 2 terms__\n",
    "- Makes TFIDF __increase with the rarity of the term in the collection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0.022276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>to</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>S</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  doc_freq       idf\n",
       "14     .        20  0.000000\n",
       "45     ,        19  0.022276\n",
       "155  the        17  0.070581\n",
       "33    to        17  0.070581\n",
       "72     S        16  0.096910"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>resales</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  doc_freq      idf\n",
       "400      128         1  1.30103\n",
       "404      990         1  1.30103\n",
       "405      500         1  1.30103\n",
       "369      689         1  1.30103\n",
       "832  resales         1  1.30103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "document_frequency = defaultdict(lambda: 0)\n",
    "for fileid in housing_articles:\n",
    "    for _word in set(reuters.words(fileid)):\n",
    "        document_frequency[_word] += 1\n",
    "\n",
    "idf_df = pd.DataFrame(list(document_frequency.items()), columns=['word', 'doc_freq'])\n",
    "idf_df['idf'] = np.log10(len(housing_articles)/idf_df['doc_freq'])\n",
    "idf_df.sort_values(by=['idf'], inplace=True)\n",
    "idf_df.head()\n",
    "idf_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we estimate score for a document D w.r.t. a query Q, __summing over tfidf scores of the word in both D and Q__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "\n",
      "Long Document TF-IDF: 1.2596373105057561\n",
      "\n",
      "Short Document TF-IDF: 2.1627272974976997\n"
     ]
    }
   ],
   "source": [
    "def tfidf_score(query_tokens, document_tokens):\n",
    "    # naive implementation\n",
    "    def tfidf(word):\n",
    "        return (1 + np.log10(document_tokens.count(word))) * idf_df[idf_df['word']==word].iloc[0]['idf']\n",
    "    \n",
    "    overlapping_tokens = set(query_tokens).intersection(set(document_tokens))\n",
    "    return sum([tfidf(_word) for _word in overlapping_tokens])\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"\\nLong Document TF-IDF:\", tfidf_score(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document TF-IDF:\", tfidf_score(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Now compute the TF-IDF score of the query to the documents:\n",
    "- Query: 'Who was the first man ever to swim around Britain?'\n",
    "- Doc1: 'Ross Edgley, at 33 - first man to swim around Britain'\n",
    "- Doc2: 'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea'\n",
    "- Doc3: 'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?'\n",
    "- Doc4: 'Welcome to the world of strongman swimming | British GQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "- Each document can be represented by a vector, where the terms are the axes of the space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Ross Edgley, at 33 - first man to swim around Britain',\n",
    "    'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea',\n",
    "    'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?',\n",
    "    'Welcome to the world of strongman swimming | British GQ'\n",
    "]\n",
    "query = 'Who was the first man ever to swim around Britain?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.feature_extraction.text:\n",
    "- __CountVectorizer__ - Convert a collection of text documents to a matrix of token counts.\n",
    "- __TfidfVectorizer__ - Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "- Two main methods __fit__ and __transform__ :\n",
    "    - __fit__ goes through the provided documents and __collects the vocabulary__\n",
    "    - __transform__ transforms __documents in text representation to a vector representation__ according to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ross': 15, 'edgley': 7, 'at': 2, '33': 0, 'first': 8, 'man': 12, 'to': 24, 'swim': 20, 'around': 1, 'britain': 3, 'circumnavigate': 6, 'spent': 18, 'months': 13, 'sea': 16, 'get': 9, 'set': 17, 'swimming': 21, 'h2omg': 11, 'can': 5, 'this': 23, 'welcome': 25, 'the': 22, 'world': 26, 'of': 14, 'strongman': 19, 'british': 4, 'gq': 10}\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(count_vectorizer.vocabulary_) # word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform produces a sparse representations of documents - only values != 0\n",
    "# we need toarray() to preview the whole lists\n",
    "count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36984162, 0.36984162, 0.29941866, 0.36984162, 0.36984162,\n",
       "        0.36984162, 0.36984162, 0.        , 0.29941866],\n",
       "       [0.        , 0.48163503, 0.38992506, 0.48163503, 0.        ,\n",
       "        0.48163503, 0.        , 0.        , 0.38992506],\n",
       "       [0.46346838, 0.        , 0.3752176 , 0.        , 0.46346838,\n",
       "        0.        , 0.46346838, 0.46346838, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.77722116, 0.62922751]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'ross': 5,\n",
       " 'edgley': 3,\n",
       " 'at': 1,\n",
       " 'man': 4,\n",
       " 'to': 8,\n",
       " 'swim': 6,\n",
       " 'around': 0,\n",
       " 'britain': 2,\n",
       " 'swimming': 7}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf_vectorizer.fit_transform(documents).toarray()\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing vector similarities\n",
    "- We would like to find documents close to a given document or the closest documents to a query\n",
    "- __Euclidean distance__? - shorter documents will be closer to each other rather than documents talking about same topic\n",
    "- __Cosine similarity__ of the angle between two documents\n",
    "    - divide each vector by its norm to achieve __unit length vectors__\n",
    "    - cosine similarity is simply the __dot product__ of two unit length vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cosine SImilarity](img/cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array([1, 0, 0, 1, 2])\n",
    "vector2 = np.array([0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40824829, 0.        , 0.        , 0.40824829, 0.81649658]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.40824829, 0.        , 0.        , 0.40824829, 0.81649658])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "preprocessing.normalize([vector1], norm='l2')\n",
    "vector1 / np.sqrt(sum(vector1**2))\n",
    "\n",
    "unit_vector1 = preprocessing.normalize([vector1], norm='l2')[0]\n",
    "unit_vector2 = preprocessing.normalize([vector2], norm='l2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(unit_vector1, unit_vector2)\n",
    "sum([unit_vector1[i]*unit_vector2[i] for i in range(len(unit_vector1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([vector1, vector2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : calculate the closes document to the query from the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the first man ever to swim around Britain?\n",
      "1st Closest document: Get Set 4 Swimming - H2OMG! Can this man swim around Britain? Score: 0.8159745583466792\n",
      "2nd Closest document: Ross Edgley, at 33 - first man to swim around Britain Score: 0.7678877104085525\n"
     ]
    }
   ],
   "source": [
    "def get_closest_documents(query, vectorizer, train_corpus_vectors, top_n=2):\n",
    "    \"\"\"Vectorizer should be fit on the documents beforehand.\n",
    "        Returns tuples of (similarity, indexes) of closest documents\"\"\"\n",
    "    # compute similarity to all sentences in the training corpus\n",
    "    similarities = cosine_similarity(vectorizer.transform([query]), train_corpus_vectors).flatten()\n",
    "    # get indexes of top n closest sentences\n",
    "    related_docs_indices = similarities.argsort()[:-top_n-1:-1]\n",
    "    # return tuples of (similarity score, document id)\n",
    "    return [(similarities[idx], idx)  for idx in related_docs_indices]\n",
    "\n",
    "train_corpus_vectors = tfidf_vectorizer.transform(documents)\n",
    "closest_documents = get_closest_documents(query, tfidf_vectorizer, train_corpus_vectors)\n",
    "print('Query:', query)\n",
    "print('1st Closest document: {} Score: {}'.format(documents[closest_documents[0][1]], closest_documents[0][0]))\n",
    "print('2nd Closest document: {} Score: {}'.format(documents[closest_documents[1][1]], closest_documents[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: using the friends corpus try to create an IR chatbot:\n",
    "- User writes a sentences and we find the __closest sentence__ from the transcript\n",
    "- We need to take __the answer__ to that sentence to make a dialogue! \n",
    "- Provide the bot with a __personality__, selecting only the tuple cues, where the answer is by a specific person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>person</th>\n",
       "      <th>gender</th>\n",
       "      <th>original_line</th>\n",
       "      <th>line</th>\n",
       "      <th>metadata</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33557</th>\n",
       "      <td>3352601</td>\n",
       "      <td>1720</td>\n",
       "      <td>JOEY</td>\n",
       "      <td>M</td>\n",
       "      <td>Joey: ... Amy! Happy birthday to you!</td>\n",
       "      <td>... Amy! Happy birthday to you!</td>\n",
       "      <td>..._... Amy_NP1 !_! Happy_JJ birthday_NN1 to_II you_PPY !_!</td>\n",
       "      <td>0613.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48622</th>\n",
       "      <td>4859101</td>\n",
       "      <td>2481</td>\n",
       "      <td>ROSS</td>\n",
       "      <td>M</td>\n",
       "      <td>Ross: This is insane, I'm not gonna make love to you just so that you'll go into labor.</td>\n",
       "      <td>This is insane, I'm not gonna make love to you just so that you'll go into labor.</td>\n",
       "      <td>This_DD1 is_VBZ insane_JJ I_PPIS1 'm_VBM not_XX gon_VVGK na_TO make_VVI love_NN1 to_II you_PPY just_RR so_CS21 that_CS22 you_PPY 'll_VM go_VVI into_II labor_NN1 ._.</td>\n",
       "      <td>0822.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56662</th>\n",
       "      <td>5663101</td>\n",
       "      <td>2860</td>\n",
       "      <td>ROSS</td>\n",
       "      <td>M</td>\n",
       "      <td>Ross: No, wait ! You guys, no, no, you can't leave! Rachel already feels bad that the cake's messed up. How do you think she's gonna feel when she comes back here and all you guys are gone?</td>\n",
       "      <td>No, wait ! You guys, no, no, you can't leave! Rachel already feels bad that the cake's messed up. How do you think she's gonna feel when she comes back here and all you guys are gone?</td>\n",
       "      <td>No_UH wait_VV0 !_! You_PPY guys_NN2 no_UH no_UH you_PPY ca_VM n't_XX leave_VVI !_! Rachel_NP1 already_RR feels_VVZ bad_JJ that_CST the_AT cake_NN1 's_VBZ messed_VVN up_RP ._. How_RRQ do_VD0 you_PPY think_VVI she_PPHS1 's_VBZ gon_VVGK na_TO feel_VVI when_RRQ she_PPHS1 comes_VVZ back_RP here_RL and_CC all_DB you_PPY guys_NN2 are_VBR gone_VVN ?_?</td>\n",
       "      <td>1004.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28431</th>\n",
       "      <td>2840001</td>\n",
       "      <td>1471</td>\n",
       "      <td>JOEY</td>\n",
       "      <td>M</td>\n",
       "      <td>Joey: Done.</td>\n",
       "      <td>Done.</td>\n",
       "      <td>Done_VDN ._.</td>\n",
       "      <td>0518.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49576</th>\n",
       "      <td>4954501</td>\n",
       "      <td>2529</td>\n",
       "      <td>JOEY</td>\n",
       "      <td>M</td>\n",
       "      <td>Joey: A little bit.</td>\n",
       "      <td>A little bit.</td>\n",
       "      <td>A_AT1 little_JJ bit_NN1 ._.</td>\n",
       "      <td>0902.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id scene_id person gender  \\\n",
       "33557  3352601  1720     JOEY   M       \n",
       "48622  4859101  2481     ROSS   M       \n",
       "56662  5663101  2860     ROSS   M       \n",
       "28431  2840001  1471     JOEY   M       \n",
       "49576  4954501  2529     JOEY   M       \n",
       "\n",
       "                                                                                                                                                                                       original_line  \\\n",
       "33557  Joey: ... Amy! Happy birthday to you!                                                                                                                                                           \n",
       "48622  Ross: This is insane, I'm not gonna make love to you just so that you'll go into labor.                                                                                                         \n",
       "56662  Ross: No, wait ! You guys, no, no, you can't leave! Rachel already feels bad that the cake's messed up. How do you think she's gonna feel when she comes back here and all you guys are gone?   \n",
       "28431  Joey: Done.                                                                                                                                                                                     \n",
       "49576  Joey: A little bit.                                                                                                                                                                             \n",
       "\n",
       "                                                                                                                                                                                          line  \\\n",
       "33557  ... Amy! Happy birthday to you!                                                                                                                                                           \n",
       "48622  This is insane, I'm not gonna make love to you just so that you'll go into labor.                                                                                                         \n",
       "56662  No, wait ! You guys, no, no, you can't leave! Rachel already feels bad that the cake's messed up. How do you think she's gonna feel when she comes back here and all you guys are gone?   \n",
       "28431  Done.                                                                                                                                                                                     \n",
       "49576  A little bit.                                                                                                                                                                             \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                        metadata  \\\n",
       "33557  ..._... Amy_NP1 !_! Happy_JJ birthday_NN1 to_II you_PPY !_!                                                                                                                                                                                                                                                                                                 \n",
       "48622  This_DD1 is_VBZ insane_JJ I_PPIS1 'm_VBM not_XX gon_VVGK na_TO make_VVI love_NN1 to_II you_PPY just_RR so_CS21 that_CS22 you_PPY 'll_VM go_VVI into_II labor_NN1 ._.                                                                                                                                                                                        \n",
       "56662  No_UH wait_VV0 !_! You_PPY guys_NN2 no_UH no_UH you_PPY ca_VM n't_XX leave_VVI !_! Rachel_NP1 already_RR feels_VVZ bad_JJ that_CST the_AT cake_NN1 's_VBZ messed_VVN up_RP ._. How_RRQ do_VD0 you_PPY think_VVI she_PPHS1 's_VBZ gon_VVGK na_TO feel_VVI when_RRQ she_PPHS1 comes_VVZ back_RP here_RL and_CC all_DB you_PPY guys_NN2 are_VBR gone_VVN ?_?   \n",
       "28431  Done_VDN ._.                                                                                                                                                                                                                                                                                                                                                \n",
       "49576  A_AT1 little_JJ bit_NN1 ._.                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "       filename  \n",
       "33557  0613.txt  \n",
       "48622  0822.txt  \n",
       "56662  1004.txt  \n",
       "28431  0518.txt  \n",
       "49576  0902.txt  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "friends_corpus = pd.read_csv(\"data/friends-final.txt\", sep='\\t')\n",
    "friends_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>There's nothing to tell! He's just some guy I work with!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOEY</td>\n",
       "      <td>C'mon, you're going out with the guy! There's gotta be something wrong with him!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Just, 'cause, I don't want her to go through what I went through with Carl- oh!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Then I look down, and I realize there's a phone... there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     person  \\\n",
       "0  MONICA     \n",
       "1  JOEY       \n",
       "2  CHANDLER   \n",
       "3  PHOEBE     \n",
       "4  PHOEBE     \n",
       "5  MONICA     \n",
       "6  CHANDLER   \n",
       "7  CHANDLER   \n",
       "8  ALL        \n",
       "9  CHANDLER   \n",
       "\n",
       "                                                                                                                  line  \n",
       "0  There's nothing to tell! He's just some guy I work with!                                                             \n",
       "1  C'mon, you're going out with the guy! There's gotta be something wrong with him!                                     \n",
       "2  Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?                                               \n",
       "3  Wait, does he eat chalk?                                                                                             \n",
       "4  Just, 'cause, I don't want her to go through what I went through with Carl- oh!                                      \n",
       "5  Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.         \n",
       "6  Sounds like a date to me.                                                                                            \n",
       "7  Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.  \n",
       "8  Oh, yeah. Had that dream.                                                                                            \n",
       "9  Then I look down, and I realize there's a phone... there.                                                            "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example conversation\n",
    "friends_corpus[friends_corpus['scene_id']=='1'][['person', 'line']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60849, 15032)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer().fit(friends_corpus.line)\n",
    "train_corpus = vectorizer.transform(friends_corpus.line)\n",
    "train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence:  C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "Its vector representation:  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "An id of a word from the sentence:  14756\n",
      "The word tf-idf score:  0.41270625402865396\n"
     ]
    }
   ],
   "source": [
    "print(\"First sentence: \", friends_corpus.line.values[1])\n",
    "print(\"Its vector representation: \", train_corpus[1].toarray())\n",
    "print(\"An id of a word from the sentence: \", vectorizer.vocabulary_['with'])\n",
    "print(\"The word tf-idf score: \", train_corpus[1].toarray()[0][vectorizer.vocabulary_['with']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8125, 5411)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the previous line, which the cue follows in the dialogue\n",
    "friends_corpus['previous_line'] = ['DUMMY PREVIOUS LINE'] + friends_corpus['line'].values[:-1].tolist()\n",
    "# select only the cues which are made by JOEY\n",
    "joey_line_tuples = friends_corpus[friends_corpus.person == 'JOEY']\n",
    "# create a vectorizer and training space of the documents in the vector space\n",
    "joey_vectorizer = TfidfVectorizer().fit(joey_line_tuples.previous_line)\n",
    "joey_train_corpus = joey_vectorizer.transform(joey_line_tuples.previous_line)\n",
    "joey_train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_utterance(cue, vectorizer,  train_corpus, top_n=5):\n",
    "    # compute similarity to all sentences in the training corpus\n",
    "    similarities = cosine_similarity(vectorizer.transform([cue]), train_corpus).flatten()\n",
    "    # get indexes of top 5 clocest sentences\n",
    "    related_docs_indices = similarities.argsort()[:-top_n:-1]\n",
    "    # return tuples of (similarity score, sentence)\n",
    "    return [(similarities[idx], joey_line_tuples['line'].values[idx]) \n",
    "                for idx in related_docs_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,\n",
       "  \"Joey Tribbiani! From the wall! Okay, maybe this will jog your memory, huh? Huh? Okay eh-ah-anyway, I'm ready to go back up on the wall I'm the star of a new TV show.\"),\n",
       " (1.0,\n",
       "  \"Oh, hi, I'm Joey. My stupid friends are buying this house. Who are you?\"),\n",
       " (0.7527478944848099, 'Me.'),\n",
       " (0.7527478944848099, \"Alright. I'll give you one hint. Warren Beatty.\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_utterance('who are you?', joey_vectorizer, joey_train_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
